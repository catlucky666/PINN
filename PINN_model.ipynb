{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db00153b-0ac3-48dd-ae6e-2cd824eda404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#0.導入套件(以前程式抓來的，可能有些用不上)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import grad\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler      #torch的太麻煩了，用sklearn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "421a601c-a59d-47fc-9df6-72fb130f27f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.讀取檔案，設定參數\n",
    "#----------------------------------------\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "#data = data.rename(columns={\"x'\": \"xp\"})     原本考量x'往後需要變成x\\'，但發現後面只出現一次，而且用不同之引號能解決'的問題，故注解掉\n",
    "X = data[['a','m','V0','n',\"x'\"]].values             \n",
    "#在 PINN ，psi_raw 不是主 label，僅是輔助監督學習之資料\n",
    "psi_data = data['psi_raw'].values.reshape(-1, 1)\n",
    "\n",
    "#2.Normalization(標準化)\n",
    "#---------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_tensor = torch.tensor(\n",
    "    X_scaled,\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True  #確保x要能微分\n",
    ").to(device)\n",
    "psi_data = torch.tensor(\n",
    "    psi_data,\n",
    "    dtype=torch.float32\n",
    ").to(device)\n",
    "E = torch.nn.Parameter(torch.tensor(1.0, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c91b03-54f3-41fa-8d10-b1f95873056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.模型\n",
    "#---------------------------------------------------------\n",
    "class PINN(nn.Module):                    #其實跟ANN一模一樣，命名成PINN\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(5 , 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128 , 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64 , 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32 , 1),\n",
    "        )\n",
    "    def forward(self , X):\n",
    "        return self.net(X)\n",
    "\n",
    "model = PINN().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + [E], lr=0.001)#因為他自己會找到ψ = 0這條最簡單的路線，強制補E使得必定要存在能量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645a934c-d756-4e55-8612-312d2fefcc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1 定義PINN專屬參數pDE殘差\n",
    "#---------------------------------------------------------\n",
    "def pde_residual(model, X):\n",
    "    psi = model(X)\n",
    "\n",
    "    # 一階導數 dψ/dx\n",
    "    psi_x = grad(\n",
    "        psi, X,\n",
    "        grad_outputs=torch.ones_like(psi),\n",
    "        create_graph=True\n",
    "    )[0][:, 4:5]   # 只取對 x 的偏導\n",
    "\n",
    "    # 二階導數 d²ψ/dx²\n",
    "    psi_xx = grad(\n",
    "        psi_x, X,\n",
    "        grad_outputs=torch.ones_like(psi_x),\n",
    "        create_graph=True\n",
    "    )[0][:, 4:5]\n",
    "    \n",
    "    # 非齊次 source term\n",
    "    x_phys = X[:, 4:5]\n",
    "    f = torch.exp(-x_phys**2)\n",
    "    epsilon0 = 1.0\n",
    "\n",
    "    residual = psi_xx + E * psi - epsilon * f\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97dd9da-d0bd-4fe2-98ea-3f3024b3de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2定義loss\n",
    "#---------------------------------------------------\n",
    "def pinn_loss(model, X, psi_data=None, lambda_data=0.1, lambda_norm=10):\n",
    "     # PDE loss\n",
    "    R = pde_residual(model, X)\n",
    "    loss_pde = torch.mean(R**2)\n",
    "    # 標準化 loss（破 ψ≡0）\n",
    "    psi = model(X)\n",
    "    norm = torch.mean(psi**2)\n",
    "    loss_norm = (norm - 1.0)**2\n",
    "    #原先data之標準化不能用在這，因為它屬於固定解，會限制模型\n",
    "    x0_index = X.shape[0] // 2      \n",
    "    psi_anchor = psi[x0_index]\n",
    "    loss_anchor = (psi_anchor - 1.0)**2\n",
    "\n",
    "    # data loss（可選）\n",
    "    if psi_data is not None:\n",
    "        loss_data = torch.mean((psi - psi_data)**2)\n",
    "    else:\n",
    "        loss_data = 0.0\n",
    "\n",
    "    loss = loss_pde+ lambda_norm * loss_norm+ lambda_data * loss_data+ 0.01 * loss_anchor\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c793ca-a9a1-4b45-adc9-82863d6e58e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 232.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 78.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mpinn_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpsi_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m, in \u001b[0;36mpinn_loss\u001b[1;34m(model, X, psi_data, lambda_data, lambda_norm)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpinn_loss\u001b[39m(model, X, psi_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lambda_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, lambda_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      4\u001b[0m      \u001b[38;5;66;03m# PDE loss\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     R \u001b[38;5;241m=\u001b[39m \u001b[43mpde_residual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     loss_pde \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(R\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# 標準化 loss（破 ψ≡0）\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mpde_residual\u001b[1;34m(model, X)\u001b[0m\n\u001b[0;32m      7\u001b[0m psi_x \u001b[38;5;241m=\u001b[39m grad(\n\u001b[0;32m      8\u001b[0m     psi, X,\n\u001b[0;32m      9\u001b[0m     grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(psi),\n\u001b[0;32m     10\u001b[0m     create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     11\u001b[0m )[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m5\u001b[39m]   \u001b[38;5;66;03m# 只取對 x 的偏導\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 二階導數 d²ψ/dx²\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m psi_xx \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpsi_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsi_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 非齊次 source term\u001b[39;00m\n\u001b[0;32m     21\u001b[0m x_phys \u001b[38;5;241m=\u001b[39m X[:, \u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m5\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-cu121\\lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    493\u001b[0m         grad_outputs_\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    509\u001b[0m     ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-cu121\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 232.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 78.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 4.訓練\n",
    "#---------------------------------------------------\n",
    "epochs = 200\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = pinn_loss(model, X_tensor, psi_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #if(i == 0 or (i + 1) % 100 == 0):\n",
    "    print(f\"Epoch{i}, loss={loss.item():.5f}, E={E.iten():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e99bf53-f78f-4c82-b681-9bd7c44e690f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and scaler saved.\n"
     ]
    }
   ],
   "source": [
    "#5.儲存\n",
    "#--------------------------------------------------\n",
    "torch.save(model.state_dict(), \"pinn_model.pth\")\n",
    "\n",
    "# 儲存 scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, \"pinn_scaler.pkl\")\n",
    "\n",
    "print(\"Model and scaler saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cac3c3-6238-4038-bd0f-b80a1deb3b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
